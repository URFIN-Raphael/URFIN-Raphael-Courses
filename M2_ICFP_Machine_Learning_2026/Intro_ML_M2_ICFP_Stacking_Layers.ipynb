{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Option 1: nn.Sequential**"
      ],
      "metadata": {
        "id": "vzbhvs3iKRuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u5_nC6NLJwoA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(2, 50), nn.ReLU(), nn.Linear(50,1), nn.Sigmoid())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This builds a Multilayer Perceptron (MLP) with input dimension 2, output dimension 1, ReLU activation function, one hidden layers with dimension 50 and a final Sigmoid layer."
      ],
      "metadata": {
        "id": "KmnYzkG8Lo6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.empty(1, 2).normal_()\n",
        "output = model(input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6G7VL1FO8bE",
        "outputId": "00146b01-5e8f-4f07-bb6d-56c195a164b0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5278]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option 2: nn.Module**"
      ],
      "metadata": {
        "id": "B-ILASIpKi3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=2, out_features=50)\n",
        "        self.fc2 = nn.Linear(in_features=50, out_features=1)\n",
        "    def forward(self,x):\n",
        "     x = self.fc1(x)\n",
        "     x = F.relu(x)\n",
        "     x = self.fc2(x)\n",
        "     return F.sigmoid(x)"
      ],
      "metadata": {
        "id": "DzidRvG2KO_3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP()\n",
        "input = torch.empty(1, 2).normal_()\n",
        "output = model(input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqJDY2BDKQwh",
        "outputId": "d5065691-aee7-42f4-c3b3-1acaa71daec2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5274]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**"
      ],
      "metadata": {
        "id": "Kriuado-TCTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Tpr5m6h1S-RL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example: Cross-Entropy Loss**\n",
        "The outputs of the neural network $z$ are transformed into probabilities $q$ using the **Softmax** function:\n",
        "\n",
        "$$q_i = \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}$$\n",
        "where $C$ is the the output dimension (number of classes).\n",
        "\n",
        "The **Cross-Entropy** measures the \"distance\" between the predicted distribution $q$ and the true distribution $p$. For a single sample, the loss is:\n",
        "\n",
        "$$\\mathcal{L}(p, q) = -\\sum_{i=1}^{C} p_i \\log(q_i)$$\n",
        "\n",
        "Since our labels are **one-hot encoded** ($p_i = 1$ for the correct class $c$, and $0$ otherwise), the formula collapses to:\n",
        "\n",
        "$$\\mathcal{L} = -\\log(q_c)$$\n"
      ],
      "metadata": {
        "id": "LRHi_ixDh0AF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer**"
      ],
      "metadata": {
        "id": "8bXYGIA6TEXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "metadata": {
        "id": "4eip4QEWTHiu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Step of Gradient Descent**"
      ],
      "metadata": {
        "id": "Y294l11JTbl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Set\n",
        "Data=...\n",
        "label=...\n",
        "\n",
        "output = model(Data)\n",
        "loss = loss_fn(output, label)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "VnfI00jbTYhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}